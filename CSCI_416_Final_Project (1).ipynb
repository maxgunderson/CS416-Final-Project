{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KVOHOwpor0Gn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# sklearn load_boston no longer works\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# i = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NDcPHwEFr7jm",
    "outputId": "9049ce49-37c7-4dba-b824-15cac55b45bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "0   0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
       "1   0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
       "2   0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
       "3   0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
       "4   0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
       "5   0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
       "6   0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
       "7   0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
       "8   0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
       "9   0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
       "10  0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467  5.0  311.0   \n",
       "11  0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267  5.0  311.0   \n",
       "12  0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509  5.0  311.0   \n",
       "13  0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075  4.0  307.0   \n",
       "14  0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619  4.0  307.0   \n",
       "15  0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986  4.0  307.0   \n",
       "16  1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986  4.0  307.0   \n",
       "17  0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579  4.0  307.0   \n",
       "18  0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965  4.0  307.0   \n",
       "19  0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965  4.0  307.0   \n",
       "\n",
       "    PTRATIO       B  LSTAT  MEDV  \n",
       "0      15.3  396.90   4.98  24.0  \n",
       "1      17.8  396.90   9.14  21.6  \n",
       "2      17.8  392.83   4.03  34.7  \n",
       "3      18.7  394.63   2.94  33.4  \n",
       "4      18.7  396.90   5.33  36.2  \n",
       "5      18.7  394.12   5.21  28.7  \n",
       "6      15.2  395.60  12.43  22.9  \n",
       "7      15.2  396.90  19.15  27.1  \n",
       "8      15.2  386.63  29.93  16.5  \n",
       "9      15.2  386.71  17.10  18.9  \n",
       "10     15.2  392.52  20.45  15.0  \n",
       "11     15.2  396.90  13.27  18.9  \n",
       "12     15.2  390.50  15.71  21.7  \n",
       "13     21.0  396.90   8.26  20.4  \n",
       "14     21.0  380.02  10.26  18.2  \n",
       "15     21.0  395.62   8.47  19.9  \n",
       "16     21.0  386.85   6.58  23.1  \n",
       "17     21.0  386.75  14.67  17.5  \n",
       "18     21.0  288.99  11.69  20.2  \n",
       "19     21.0  390.95  11.28  18.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the data and target np arrays into a dataframe with appropriate column names\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "data_values = pd.DataFrame(data, columns=column_names)\n",
    "data_values['MEDV'] = target\n",
    "# data_values.drop('B', axis = 1,inplace = True)\n",
    "# print(data_values.columns)\n",
    "\n",
    "data_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-H6Ht-18c9nH"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7pDW_5Oznmjt"
   },
   "outputs": [],
   "source": [
    "# declaring the features and target and scaling features\n",
    "\n",
    "X = data_values[column_names]\n",
    "y = data_values['MEDV']\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "X = scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rfDJ9eplpQWR"
   },
   "outputs": [],
   "source": [
    "# creating testing and training splits\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KicCQ8OUwB1w"
   },
   "outputs": [],
   "source": [
    "# helper function that will declare the model with the desired parameters, fit it, and print the mse\n",
    "def get_mse(learning_rate, regularization):\n",
    "  sgdr = SGDRegressor(eta0=learning_rate, penalty=regularization)\n",
    "  sgdr.fit(X_train, y_train)\n",
    "  y_pred = sgdr.predict(X_test)\n",
    "  mse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "  print(f'Mean squared error (L{regularization[1:]} regularization & {learning_rate} learning rate):', mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgWWzvukqxGb",
    "outputId": "d9351821-50dc-4d7d-8dad-614ced236abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (L1 regularization & 0.001 learning rate): 25.744938544289063\n",
      "Mean squared error (L1 regularization & 0.01 learning rate): 25.0271561736867\n",
      "Mean squared error (L1 regularization & 0.1 learning rate): 25.551435637185893\n"
     ]
    }
   ],
   "source": [
    "# we want to test different learning rates (0.001, 0.01, 0.1) with both L1 and L2 regularization\n",
    "l1 = 'l1'\n",
    "# L1 regularization with eta = 0.001\n",
    "get_mse(0.001, l1)\n",
    "# L1 regularization with eta = 0.01\n",
    "get_mse(0.01, l1)\n",
    "# L1 regularization with eta = 0.1\n",
    "get_mse(0.1, l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWZxTtpcw65t",
    "outputId": "eae80f04-419a-469f-bbd4-f298235048cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (L2 regularization & 0.001 learning rate): 25.715080116800127\n",
      "Mean squared error (L2 regularization & 0.01 learning rate): 24.988038227029527\n",
      "Mean squared error (L2 regularization & 0.1 learning rate): 29.3445161935919\n"
     ]
    }
   ],
   "source": [
    "# now do the above process with l2 regularization\n",
    "l2 = 'l2'\n",
    "# L2 regularization with eta = 0.001\n",
    "get_mse(0.001, l2)\n",
    "# L2 regularization with eta = 0.01\n",
    "get_mse(0.01, l2)\n",
    "# L2 regularization with eta = 0.1\n",
    "get_mse(0.1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def get_avg_mse(learningRate, regularizationType):\n",
    "    errors = []\n",
    "    for i in range(101):\n",
    "        # Create new model and train it in each iteration\n",
    "        model = SGDRegressor(eta0=learningRate, penalty=regularizationType)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Predict MSE \n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "        errors.append(mse)\n",
    "    \n",
    "    # print(errors) # testing print to see if errors are all different \n",
    "    return statistics.mean(errors) # Return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Block for get_ave_mse\n",
    "model = SGDRegressor(eta0=0.001, penalty='l1')\n",
    "model.fit(X_train, y_train)\n",
    "avg_mse1 = get_avg_mse(0.001, 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creates a visualization of the 6 combinations of MSE \n",
    "def visualize_MSE(m1, m2, m3, m4, m5, m6):\n",
    "    # Example average MSE values for visualization \n",
    "    results = {\n",
    "        'L1': {'0.001': m1, '0.01': m2, '0.1': m3},\n",
    "        'L2': {'0.001': m4, '0.01': m5, '0.1': m6}\n",
    "    }\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    regularizations = ['L1', 'L2']\n",
    "    learning_rates = ['0.001', '0.01', '0.1']\n",
    "    x_labels = [f'{reg}-{lr}' for reg in regularizations for lr in learning_rates]\n",
    "    mse_values = [results[reg][lr] for reg in regularizations for lr in learning_rates]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(x_labels, mse_values, color='skyblue')\n",
    "    plt.xlabel('Regularization-Learning Rate Combination')\n",
    "    plt.ylabel('Average MSE')\n",
    "    plt.title('Average MSE for Different Regularization and Learning Rate Combinations')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFFc-jg9ue-l",
    "outputId": "08498c3c-e395-402a-c3ac-bbe71c82ba6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average mse after 100 trials for SGD regression with L1 regulzarization and a learning rate of 0.001: 25.73474393824135\n",
      "The average mse after 100 trials for SGD regression with L1 regulzarization and a learning rate of 0.01: 24.882109197214927\n",
      "The average mse after 100 trials for SGD regression with L1 regulzarization and a learning rate of 0.1: 25.711164093238615\n",
      "The average mse after 100 trials for SGD regression with L2 regulzarization and a learning rate of 0.001: 25.73446875877637\n",
      "The average mse after 100 trials for SGD regression with L2 regulzarization and a learning rate of 0.01: 24.85332650103069\n",
      "The average mse after 100 trials for SGD regression with L2 regulzarization and a learning rate of 0.1: 25.471074178214366\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now each of those regularization/learning rate combos will be run 100 times and the errors of the\n",
    "# trials will be averaged to determine more accurate error values\n",
    "\n",
    "avg_mse1 = get_avg_mse(0.001, 'l1')\n",
    "print('The average mse after 100 trials for SGD regression with L1 regulzarization and a learning rate of 0.001:', avg_mse1)\n",
    "\n",
    "avg_mse2 = get_avg_mse(0.01, 'l1')\n",
    "print('The average mse after 100 trials for SGD regression with L1 regulzarization and a learning rate of 0.01:', avg_mse2)\n",
    "\n",
    "avg_mse3 = get_avg_mse(0.1, 'l1')\n",
    "print('The average mse after 100 trials for SGD regression with L1 regulzarization and a learning rate of 0.1:', avg_mse3)\n",
    "\n",
    "avg_mse4 = get_avg_mse(0.001, 'l1')\n",
    "print('The average mse after 100 trials for SGD regression with L2 regulzarization and a learning rate of 0.001:', avg_mse4)\n",
    "\n",
    "avg_mse5 = get_avg_mse(0.01, 'l2')\n",
    "print('The average mse after 100 trials for SGD regression with L2 regulzarization and a learning rate of 0.01:', avg_mse5)\n",
    "\n",
    "avg_mse6 = get_avg_mse(0.1, 'l2')\n",
    "print('The average mse after 100 trials for SGD regression with L2 regulzarization and a learning rate of 0.1:', avg_mse6)\n",
    "\n",
    "visualize_MSE(avg_mse1, avg_mse2, avg_mse3, avg_mse4, avg_mse5, avg_mse6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6jKPYR5BvzTq"
   },
   "outputs": [],
   "source": [
    "# now train AdaBoost Classifier on the data and compare the results\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "C0svNaTlvzTq"
   },
   "outputs": [],
   "source": [
    "# declare decision stumps to use in AdaBoost\n",
    "stump = DecisionTreeRegressor(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "s53DUfwmvzTr"
   },
   "outputs": [],
   "source": [
    "# declare AdaBoost model\n",
    "adaboost = AdaBoostRegressor(estimator=stump, n_estimators=5000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itD7yq9dvzTr",
    "outputId": "8f2394c4-5f07-4c73-a2d4-be7ad34207ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (AdaBoost Regression with decision stumps): 50.968416349237735\n"
     ]
    }
   ],
   "source": [
    "# get predictions from AdaBoost model and get MSE to compare with linear regression models\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "# using AdaBoost with strict decision stumps (max_depth = 1) performs poorly as the data has too\n",
    "# many features and the stumps are not able to provide enough information\n",
    "print('Mean squared error (AdaBoost Regression with decision stumps):', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SODwaOd0vqKE",
    "outputId": "3bff18ef-214f-4919-a6fa-0a445e16ed3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average mse after 100 trials for AdaBoost regression with decision stumps: 50.968416349237735\n"
     ]
    }
   ],
   "source": [
    "# get the average mse after 100 trials\n",
    "print('The average mse after 100 trials for AdaBoost regression with decision stumps:', get_avg_mse(adaboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnuxhafivzTr",
    "outputId": "e8f48cd3-bdd4-4ddc-c08d-9736153345c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (AdaBoost Regression with decision trees): 12.918945116547334\n"
     ]
    }
   ],
   "source": [
    "# try AdaBoost with more complex base estimator\n",
    "tree = DecisionTreeRegressor(max_depth=4)\n",
    "adaboost = AdaBoostRegressor(estimator=tree, n_estimators=5000, learning_rate=0.1)\n",
    "\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "# using AdaBoost with strict decision trees allow AdaBoost to gather more information\n",
    "# and thus make better predictions\n",
    "print('Mean squared error (AdaBoost Regression with decision trees):', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLd_90Fjv9Vp",
    "outputId": "97ef2ae1-2370-4115-a311-5e799567992f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average mse after 100 trials for AdaBoost regression with decision trees: 12.918945116547334\n"
     ]
    }
   ],
   "source": [
    "# get the average mse after 100 trials\n",
    "print('The average mse after 100 trials for AdaBoost regression with decision trees:', get_avg_mse(adaboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AxC9jSlSvzTr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# now try training a neural network to perform the regression\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, InputLayer, BatchNormalization\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regularizers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# now try training a neural network to perform the regression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Pth36OjUwcEn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 20:00:29.560813: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# define the neural network and add an input + hidden layers\n",
    "nn = Sequential()\n",
    "nn.add(InputLayer((X_train.shape[1],)))\n",
    "# add a lay with regularization\n",
    "nn.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# Batch normalization to normalize the outputs from each hidden layer before passing them along\n",
    "# Batch normalization ensures that the data distribution remains consistent\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dense(8, activation='relu'))\n",
    "nn.add(BatchNormalization())\n",
    "# linear activation function for output layer in regression\n",
    "nn.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ap0EkxOwzQaW",
    "outputId": "4bc63c01-1409-4859-b659-b6fbe8e0a80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 565.1252 - mse: 563.6611 - val_loss: 577.2261 - val_mse: 575.7692\n",
      "Epoch 2/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 583.8647 - mse: 582.4086 - val_loss: 555.8215 - val_mse: 554.3700\n",
      "Epoch 3/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 559.4481 - mse: 557.9983 - val_loss: 535.8368 - val_mse: 534.3917\n",
      "Epoch 4/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 528.0127 - mse: 526.5696 - val_loss: 516.2354 - val_mse: 514.7963\n",
      "Epoch 5/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 496.6569 - mse: 495.2192 - val_loss: 502.5255 - val_mse: 501.0912\n",
      "Epoch 6/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 514.3332 - mse: 512.9005 - val_loss: 485.1458 - val_mse: 483.7179\n",
      "Epoch 7/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 507.1411 - mse: 505.7148 - val_loss: 466.5573 - val_mse: 465.1356\n",
      "Epoch 8/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 471.2674 - mse: 469.8467 - val_loss: 449.7429 - val_mse: 448.3273\n",
      "Epoch 9/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 432.4881 - mse: 431.0741 - val_loss: 433.2188 - val_mse: 431.8098\n",
      "Epoch 10/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 379.1864 - mse: 377.7793 - val_loss: 408.5960 - val_mse: 407.1934\n",
      "Epoch 11/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 346.7776 - mse: 345.3766 - val_loss: 389.2301 - val_mse: 387.8342\n",
      "Epoch 12/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 360.6979 - mse: 359.3028 - val_loss: 360.8385 - val_mse: 359.4478\n",
      "Epoch 13/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 312.7074 - mse: 311.3193 - val_loss: 332.7645 - val_mse: 331.3822\n",
      "Epoch 14/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 312.6938 - mse: 311.3128 - val_loss: 312.1928 - val_mse: 310.8183\n",
      "Epoch 15/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 286.1625 - mse: 284.7900 - val_loss: 278.8854 - val_mse: 277.5180\n",
      "Epoch 16/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 273.3289 - mse: 271.9627 - val_loss: 249.8681 - val_mse: 248.5084\n",
      "Epoch 17/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 228.3183 - mse: 226.9601 - val_loss: 228.6778 - val_mse: 227.3232\n",
      "Epoch 18/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 209.6503 - mse: 208.2966 - val_loss: 204.8421 - val_mse: 203.4924\n",
      "Epoch 19/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 204.1465 - mse: 202.7983 - val_loss: 184.6808 - val_mse: 183.3359\n",
      "Epoch 20/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 183.8709 - mse: 182.5272 - val_loss: 170.1888 - val_mse: 168.8505\n",
      "Epoch 21/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 173.5415 - mse: 172.2054 - val_loss: 147.3801 - val_mse: 146.0506\n",
      "Epoch 22/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 138.1521 - mse: 136.8245 - val_loss: 127.7129 - val_mse: 126.3901\n",
      "Epoch 23/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 119.4954 - mse: 118.1749 - val_loss: 110.5946 - val_mse: 109.2779\n",
      "Epoch 24/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 111.9303 - mse: 110.6124 - val_loss: 93.8343 - val_mse: 92.5188\n",
      "Epoch 25/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 96.2983 - mse: 94.9845 - val_loss: 89.0999 - val_mse: 87.7901\n",
      "Epoch 26/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 81.8471 - mse: 80.5379 - val_loss: 74.8006 - val_mse: 73.4942\n",
      "Epoch 27/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 74.0586 - mse: 72.7534 - val_loss: 65.9482 - val_mse: 64.6479\n",
      "Epoch 28/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 54.8477 - mse: 53.5476 - val_loss: 57.8666 - val_mse: 56.5682\n",
      "Epoch 29/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 60.1859 - mse: 58.8887 - val_loss: 46.5918 - val_mse: 45.2969\n",
      "Epoch 30/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 42.7461 - mse: 41.4518 - val_loss: 39.5770 - val_mse: 38.2858\n",
      "Epoch 31/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 49.3141 - mse: 48.0244 - val_loss: 36.0902 - val_mse: 34.8025\n",
      "Epoch 32/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 33.3869 - mse: 32.0999 - val_loss: 32.1936 - val_mse: 30.9098\n",
      "Epoch 33/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29.0663 - mse: 27.7832 - val_loss: 30.2721 - val_mse: 28.9920\n",
      "Epoch 34/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32.7262 - mse: 31.4478 - val_loss: 29.5553 - val_mse: 28.2801\n",
      "Epoch 35/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 28.3820 - mse: 27.1075 - val_loss: 26.6991 - val_mse: 25.4254\n",
      "Epoch 36/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29.0166 - mse: 27.7435 - val_loss: 25.8034 - val_mse: 24.5315\n",
      "Epoch 37/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 37.6441 - mse: 36.3728 - val_loss: 23.5234 - val_mse: 22.2540\n",
      "Epoch 38/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32.2380 - mse: 30.9687 - val_loss: 21.8795 - val_mse: 20.6117\n",
      "Epoch 39/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 28.7210 - mse: 27.4544 - val_loss: 21.9092 - val_mse: 20.6442\n",
      "Epoch 40/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.6496 - mse: 22.3851 - val_loss: 21.3837 - val_mse: 20.1226\n",
      "Epoch 41/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.3748 - mse: 23.1150 - val_loss: 21.2394 - val_mse: 19.9813\n",
      "Epoch 42/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.2607 - mse: 25.0033 - val_loss: 20.0039 - val_mse: 18.7484\n",
      "Epoch 43/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.3797 - mse: 23.1242 - val_loss: 19.8802 - val_mse: 18.6261\n",
      "Epoch 44/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 36.0466 - mse: 34.7932 - val_loss: 19.5155 - val_mse: 18.2645\n",
      "Epoch 45/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.6714 - mse: 20.4209 - val_loss: 19.3081 - val_mse: 18.0597\n",
      "Epoch 46/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.3314 - mse: 22.0840 - val_loss: 20.0651 - val_mse: 18.8202\n",
      "Epoch 47/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.0875 - mse: 16.8432 - val_loss: 19.5535 - val_mse: 18.3109\n",
      "Epoch 48/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.2551 - mse: 24.0133 - val_loss: 19.2020 - val_mse: 17.9635\n",
      "Epoch 49/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.0103 - mse: 25.7723 - val_loss: 19.0261 - val_mse: 17.7898\n",
      "Epoch 50/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.0829 - mse: 18.8471 - val_loss: 18.5910 - val_mse: 17.3580\n",
      "Epoch 51/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.5875 - mse: 22.3551 - val_loss: 18.0050 - val_mse: 16.7739\n",
      "Epoch 52/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.0752 - mse: 19.8448 - val_loss: 18.5111 - val_mse: 17.2816\n",
      "Epoch 53/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.9625 - mse: 21.7335 - val_loss: 18.3839 - val_mse: 17.1573\n",
      "Epoch 54/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.0299 - mse: 25.8039 - val_loss: 18.1579 - val_mse: 16.9334\n",
      "Epoch 55/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.1108 - mse: 19.8870 - val_loss: 17.8792 - val_mse: 16.6580\n",
      "Epoch 56/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29.0380 - mse: 27.8178 - val_loss: 17.7489 - val_mse: 16.5315\n",
      "Epoch 57/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.7105 - mse: 19.4939 - val_loss: 17.4863 - val_mse: 16.2730\n",
      "Epoch 58/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29.6257 - mse: 28.4135 - val_loss: 17.8368 - val_mse: 16.6285\n",
      "Epoch 59/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.9816 - mse: 17.7737 - val_loss: 17.2872 - val_mse: 16.0800\n",
      "Epoch 60/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.7136 - mse: 25.5069 - val_loss: 17.5857 - val_mse: 16.3805\n",
      "Epoch 61/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 28.5863 - mse: 27.3818 - val_loss: 18.0806 - val_mse: 16.8776\n",
      "Epoch 62/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25.5527 - mse: 24.3505 - val_loss: 17.9632 - val_mse: 16.7642\n",
      "Epoch 63/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.1682 - mse: 22.9704 - val_loss: 18.0257 - val_mse: 16.8294\n",
      "Epoch 64/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.2126 - mse: 23.0169 - val_loss: 18.0432 - val_mse: 16.8489\n",
      "Epoch 65/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.2422 - mse: 17.0479 - val_loss: 17.7267 - val_mse: 16.5323\n",
      "Epoch 66/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.5215 - mse: 23.3273 - val_loss: 17.6051 - val_mse: 16.4124\n",
      "Epoch 67/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.8571 - mse: 22.6651 - val_loss: 18.1348 - val_mse: 16.9450\n",
      "Epoch 68/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 28.5583 - mse: 27.3694 - val_loss: 17.4744 - val_mse: 16.2872\n",
      "Epoch 69/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.9110 - mse: 22.7248 - val_loss: 17.0420 - val_mse: 15.8581\n",
      "Epoch 70/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.7329 - mse: 22.5496 - val_loss: 17.0008 - val_mse: 15.8206\n",
      "Epoch 71/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.1816 - mse: 21.0025 - val_loss: 17.1436 - val_mse: 15.9674\n",
      "Epoch 72/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.1328 - mse: 24.9575 - val_loss: 17.0869 - val_mse: 15.9137\n",
      "Epoch 73/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.3193 - mse: 21.1469 - val_loss: 16.7325 - val_mse: 15.5616\n",
      "Epoch 74/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.6301 - mse: 20.4600 - val_loss: 16.7443 - val_mse: 15.5770\n",
      "Epoch 75/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.5334 - mse: 17.3667 - val_loss: 17.4934 - val_mse: 16.3294\n",
      "Epoch 76/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.4275 - mse: 24.2646 - val_loss: 16.8345 - val_mse: 15.6733\n",
      "Epoch 77/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.5988 - mse: 20.4383 - val_loss: 16.7862 - val_mse: 15.6275\n",
      "Epoch 78/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.2597 - mse: 23.1015 - val_loss: 17.2336 - val_mse: 16.0776\n",
      "Epoch 79/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.8123 - mse: 18.6578 - val_loss: 16.5838 - val_mse: 15.4296\n",
      "Epoch 80/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 28.2544 - mse: 27.1010 - val_loss: 16.4247 - val_mse: 15.2740\n",
      "Epoch 81/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.9861 - mse: 16.8360 - val_loss: 16.6082 - val_mse: 15.4599\n",
      "Epoch 82/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.1470 - mse: 21.9991 - val_loss: 16.7613 - val_mse: 15.6151\n",
      "Epoch 83/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.6168 - mse: 17.4711 - val_loss: 17.0777 - val_mse: 15.9340\n",
      "Epoch 84/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.9076 - mse: 17.7647 - val_loss: 17.6450 - val_mse: 16.5036\n",
      "Epoch 85/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.6176 - mse: 23.4770 - val_loss: 16.5506 - val_mse: 15.4123\n",
      "Epoch 86/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.3910 - mse: 18.2533 - val_loss: 16.9614 - val_mse: 15.8255\n",
      "Epoch 87/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.5011 - mse: 19.3657 - val_loss: 16.8146 - val_mse: 15.6806\n",
      "Epoch 88/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.7146 - mse: 24.5812 - val_loss: 16.3393 - val_mse: 15.2079\n",
      "Epoch 89/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.8962 - mse: 20.7652 - val_loss: 15.6121 - val_mse: 14.4828\n",
      "Epoch 90/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.3718 - mse: 20.2421 - val_loss: 15.1978 - val_mse: 14.0680\n",
      "Epoch 91/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.0638 - mse: 19.9348 - val_loss: 16.0679 - val_mse: 14.9420\n",
      "Epoch 92/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.3248 - mse: 20.1997 - val_loss: 15.7906 - val_mse: 14.6670\n",
      "Epoch 93/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.6578 - mse: 20.5349 - val_loss: 14.9384 - val_mse: 13.8172\n",
      "Epoch 94/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.5775 - mse: 16.4566 - val_loss: 15.5017 - val_mse: 14.3826\n",
      "Epoch 95/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.5052 - mse: 19.3870 - val_loss: 15.9990 - val_mse: 14.8821\n",
      "Epoch 96/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.4596 - mse: 21.3440 - val_loss: 15.9517 - val_mse: 14.8379\n",
      "Epoch 97/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.5631 - mse: 19.4504 - val_loss: 15.5798 - val_mse: 14.4694\n",
      "Epoch 98/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.7946 - mse: 22.6852 - val_loss: 15.3789 - val_mse: 14.2719\n",
      "Epoch 99/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.8823 - mse: 25.7756 - val_loss: 15.8159 - val_mse: 14.7097\n",
      "Epoch 100/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.8715 - mse: 25.7662 - val_loss: 16.6678 - val_mse: 15.5653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f933c8f9c90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the neural network and fit it to the data\n",
    "nn.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "nn.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E77YIG9n1e9y",
    "outputId": "bffb6d42-5630-49fe-84cd-231fb8610b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    }
   ],
   "source": [
    "# now get predictions on the test data and mse\n",
    "y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYL9sLOy7B6K",
    "outputId": "64913b2b-bf08-419b-de75-41d26bdb1146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (neural network): 12.985176286820806\n"
     ]
    }
   ],
   "source": [
    "# it is clear that the neural network performed well but also required the most computational resources\n",
    "# the neural network was able to capture nuances and nonlinearity in the data that the linear regression\n",
    "# may not have been able to. However, AdaBoost with decision trees performed better and was both easier to\n",
    "# write and computationally more efficient than a neural network\n",
    "mse = mean_squared_error(y_pred=y_pred, y_true=y_test)\n",
    "print('Mean squared error (neural network):', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNNMMCYMwDTM",
    "outputId": "88832e0c-5fb1-4e3b-924a-02060cf27dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "The average mse after 100 trials for a neural network: 12.985176286820806\n"
     ]
    }
   ],
   "source": [
    "# get the average mse after 100 trials\n",
    "print('The average mse after 100 trials for a neural network:', get_avg_mse(nn))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
